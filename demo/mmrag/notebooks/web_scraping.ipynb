{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e254cf03-49fc-4051-a4df-3a8e4e7d2688",
   "metadata": {
    "id": "e254cf03-49fc-4051-a4df-3a8e4e7d2688"
   },
   "source": [
    "---\n",
    "title: Web scraping\n",
    "sidebar_class_name: hidden\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605e7f7",
   "metadata": {
    "id": "6605e7f7"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/use_cases/web_scraping.ipynb)\n",
    "\n",
    "## Use case\n",
    "\n",
    "[Web research](https://blog.langchain.dev/automating-web-research/) is one of the killer LLM applications:\n",
    "\n",
    "* Users have [highlighted it](https://twitter.com/GregKamradt/status/1679913813297225729?s=20) as one of his top desired AI tools.\n",
    "* OSS repos like [gpt-researcher](https://github.com/assafelovic/gpt-researcher) are growing in popularity.\n",
    "\n",
    "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/web_scraping.png?raw=1)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Gathering content from the web has a few components:\n",
    "\n",
    "* `Search`: Query to url (e.g., using `GoogleSearchAPIWrapper`).\n",
    "* `Loading`: Url to HTML  (e.g., using `AsyncHtmlLoader`, `AsyncChromiumLoader`, etc).\n",
    "* `Transforming`: HTML to formatted text (e.g., using `HTML2Text` or `Beautiful Soup`).\n",
    "\n",
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1803c182",
   "metadata": {
    "collapsed": true,
    "id": "1803c182",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting playwright\n",
      "  Downloading playwright-1.43.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting greenlet==3.0.3 (from playwright)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting pyee==11.1.0 (from playwright)\n",
      "  Downloading pyee-11.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/alfred/anaconda3/envs/dui/lib/python3.11/site-packages (from pyee==11.1.0->playwright) (4.8.0)\n",
      "Downloading playwright-1.43.0-py3-none-manylinux1_x86_64.whl (37.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.3/37.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (620 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.0/620.0 kB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyee-11.1.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyee, greenlet, playwright\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 3.0.0\n",
      "    Uninstalling greenlet-3.0.0:\n",
      "      Successfully uninstalled greenlet-3.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-index-legacy 0.9.48 requires networkx>=3.0, but you have networkx 2.6.3 which is incompatible.\n",
      "llama-index-core 0.10.15 requires networkx>=3.0, but you have networkx 2.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed greenlet-3.0.3 playwright-1.43.0 pyee-11.1.0\n"
     ]
    }
   ],
   "source": [
    "#pip install -q langchain-openai langchain playwright beautifulsou!4\n",
    "!pip install playwright --upgrade\n",
    "\n",
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "# import dotenv\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50741083",
   "metadata": {
    "id": "50741083"
   },
   "source": [
    "Scraping HTML content using a headless instance of Chromium.\n",
    "\n",
    "* The async nature of the scraping process is handled using Python's asyncio library.\n",
    "* The actual interaction with the web pages is handled by Playwright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd457cb1",
   "metadata": {
    "id": "cd457cb1"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load HTML\u001b[39;00m\n\u001b[1;32m      5\u001b[0m loader \u001b[38;5;241m=\u001b[39m AsyncChromiumLoader([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.wsj.com\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m html \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/langchain_community/document_loaders/chromium.py:82\u001b[0m, in \u001b[0;36mAsyncChromiumLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mLazily load text content from the provided URLs.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murls:\n\u001b[0;32m---> 82\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascrape_playwright(url))\n\u001b[1;32m     83\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: url}\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mhtml_content, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/asyncio/runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncChromiumLoader\n",
    "from langchain_community.document_transformers import BeautifulSoupTransformer\n",
    "\n",
    "# Load HTML\n",
    "loader = AsyncChromiumLoader([\"https://www.wsj.com\"])\n",
    "html = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a879806",
   "metadata": {
    "id": "2a879806"
   },
   "source": [
    "Scrape text content tags such as `<p>, <li>, <div>, and <a>` tags from the HTML content:\n",
    "\n",
    "* `<p>`: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases.\n",
    "\n",
    "* `<li>`: The list item tag. It is used within ordered (`<ol>`) and unordered (`<ul>`) lists to define individual items within the list.\n",
    "\n",
    "* `<div>`: The division tag. It is a block-level element used to group other inline or block-level elements.\n",
    "\n",
    "* `<a>`: The anchor tag. It is used to define hyperlinks.\n",
    "\n",
    "* `<span>`:  an inline container used to mark up a part of a text, or a part of a document.\n",
    "\n",
    "For many news websites (e.g., WSJ, CNN), headlines and summaries are all in `<span>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f206b",
   "metadata": {
    "id": "141f206b"
   },
   "outputs": [],
   "source": [
    "# Transform\n",
    "bs_transformer = BeautifulSoupTransformer()\n",
    "docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb234",
   "metadata": {
    "id": "73ddb234",
    "outputId": "3e5c253e-3410-43cb-986c-df4369dbf2bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English EditionEnglish中文 (Chinese)日本語 (Japanese) More Other Products from WSJBuy Side from WSJWSJ ShopWSJ Wine Other Products from WSJ Search Quotes and Companies Search Quotes and Companies 0.15% 0.03% 0.12% -0.42% 4.102% -0.69% -0.25% -0.15% -1.82% 0.24% 0.19% -1.10% About Evan His Family Reflects His Reporting How You Can Help Write a Message Life in Detention Latest News Get Email Updates Four Americans Released From Iranian Prison The Americans will remain under house arrest until they are '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Result\n",
    "docs_transformed[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26d185",
   "metadata": {
    "id": "7d26d185"
   },
   "source": [
    "These `Documents` now are staged for downstream usage in various LLM apps, as discussed below.\n",
    "\n",
    "## Loader\n",
    "\n",
    "### AsyncHtmlLoader\n",
    "\n",
    "The [AsyncHtmlLoader](/docs/integrations/document_loaders/async_html) uses the `aiohttp` library to make asynchronous HTTP requests, suitable for simpler and lightweight scraping.\n",
    "\n",
    "### AsyncChromiumLoader\n",
    "\n",
    "The [AsyncChromiumLoader](/docs/integrations/document_loaders/async_chromium) uses Playwright to launch a Chromium instance, which can handle JavaScript rendering and more complex web interactions.\n",
    "\n",
    "Chromium is one of the browsers supported by Playwright, a library used to control browser automation.\n",
    "\n",
    "Headless mode means that the browser is running without a graphical user interface, which is commonly used for web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8941e855",
   "metadata": {
    "id": "8941e855"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#######################################################################################################################################| 2/2 [00:00<00:00, 16.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f4bf0",
   "metadata": {
    "id": "e47f4bf0"
   },
   "source": [
    "## Transformer\n",
    "\n",
    "### HTML2Text\n",
    "\n",
    "[HTML2Text](/docs/integrations/document_transformers/html2text) provides a straightforward conversion of HTML content into plain text (with markdown-like formatting) without any specific tag manipulation.\n",
    "\n",
    "It's best suited for scenarios where the goal is to extract human-readable text without needing to manipulate specific HTML elements.\n",
    "\n",
    "### Beautiful Soup\n",
    "\n",
    "Beautiful Soup offers more fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.\n",
    "\n",
    "It's suited for cases where you want to extract specific information and clean up the HTML content according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23ff20-6698-4d78-a0cb-e3dfe95a2059",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install hml2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a7e2a8",
   "metadata": {
    "id": "99a7e2a8",
    "outputId": "141f35eb-4e48-42c9-ea7a-37167ac3baec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|######################################################################################################################################| 2/2 [00:00<00:00, 37.05it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import AsyncHtmlLoader\n",
    "\n",
    "urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\n",
    "loader = AsyncHtmlLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cd3e8d",
   "metadata": {
    "id": "a2cd3e8d",
    "outputId": "e4b1b4d9-009b-41eb-aeaa-b60736f8989b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip to main content  Skip to navigation\\n\\n<\\n\\n>\\n\\nMenu\\n\\n## ESPN\\n\\n  *   *   * scores\\n\\n  * NFL\\n  * NBA\\n  * MLB\\n  * NHL\\n  * Soccer\\n  * MMA\\n  * …\\n\\n    * NCAAF\\n    * Sports Betting\\n    * Boxing\\n    * CFL\\n    * NCAA\\n    * Cricket\\n    * F1\\n    * Golf\\n    * Horse\\n    * LLWS\\n    * NASCAR\\n    * NBA G League\\n    * NCAAM\\n    * NCAAW\\n    * Olympic Sports\\n    * PLL\\n    * Professional Wrestling\\n    * Racing\\n    * RN BB\\n    * RN FB\\n    * Rugby\\n    * Tennis\\n    * WNBA\\n    * X Games\\n    * UFL\\n\\n  * More ESPN\\n  * Fan'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "\n",
    "html2text = Html2TextTransformer()\n",
    "docs_transformed = html2text.transform_documents(docs)\n",
    "docs_transformed[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef9861",
   "metadata": {
    "id": "8aef9861"
   },
   "source": [
    "## Scraping with extraction\n",
    "\n",
    "### LLM with function calling\n",
    "\n",
    "Web scraping is challenging for many reasons.\n",
    "\n",
    "One of them is the changing nature of modern websites' layouts and content, which requires modifying scraping scripts to accommodate the changes.\n",
    "\n",
    "Using Function (e.g., OpenAI) with an extraction chain, we avoid having to change your code constantly when websites change.\n",
    "\n",
    "We're using `gpt-3.5-turbo-0613` to guarantee access to OpenAI Functions feature (although this might be available to everyone by time of writing).\n",
    "\n",
    "We're also keeping `temperature` at `0` to keep randomness of the LLM down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d49f6f",
   "metadata": {
    "id": "52d49f6f"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5757ce",
   "metadata": {
    "id": "fc5757ce"
   },
   "source": [
    "### Define a schema\n",
    "\n",
    "Next, you define a schema to specify what kind of data you want to extract.\n",
    "\n",
    "Here, the key names matter as they tell the LLM what kind of information they want.\n",
    "\n",
    "So, be as detailed as possible.\n",
    "\n",
    "In this example, we want to scrape only news article's name and summary from The Wall Street Journal website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95506f8e",
   "metadata": {
    "id": "95506f8e"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"news_article_title\": {\"type\": \"string\"},\n",
    "        \"news_article_summary\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"news_article_title\", \"news_article_summary\"],\n",
    "}\n",
    "\n",
    "\n",
    "def extract(content: str, schema: dict):\n",
    "    return create_extraction_chain(schema=schema, llm=llm).run(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7de42",
   "metadata": {
    "id": "97f7de42"
   },
   "source": [
    "### Run the web scraper w/ BeautifulSoup\n",
    "\n",
    "As shown above, we'll be using `BeautifulSoupTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977560ba",
   "metadata": {
    "collapsed": true,
    "id": "977560ba",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a0ca3063-54f5-4a82-9a02-2f8ecb7b58de",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Connection closed while reading from the driver",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m extracted_content\n\u001b[1;32m     29\u001b[0m urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.wsj.com\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m extracted_content \u001b[38;5;241m=\u001b[39m scrape_with_playwright(urls, schema\u001b[38;5;241m=\u001b[39mschema)\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mscrape_with_playwright\u001b[0;34m(urls, schema)\u001b[0m\n\u001b[1;32m      8\u001b[0m loader \u001b[38;5;241m=\u001b[39m AsyncChromiumLoader(urls)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#nest_asyncio.apply()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m     11\u001b[0m bs_transformer \u001b[38;5;241m=\u001b[39m BeautifulSoupTransformer()\n\u001b[1;32m     12\u001b[0m docs_transformed \u001b[38;5;241m=\u001b[39m bs_transformer\u001b[38;5;241m.\u001b[39mtransform_documents(\n\u001b[1;32m     13\u001b[0m     docs, tags_to_extract\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/langchain_community/document_loaders/chromium.py:82\u001b[0m, in \u001b[0;36mAsyncChromiumLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mLazily load text content from the provided URLs.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murls:\n\u001b[0;32m---> 82\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascrape_playwright(url))\n\u001b[1;32m     83\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: url}\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mhtml_content, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/langchain_community/document_loaders/chromium.py:58\u001b[0m, in \u001b[0;36mAsyncChromiumLoader.ascrape_playwright\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     56\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting scraping...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m async_playwright() \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m     59\u001b[0m     browser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m p\u001b[38;5;241m.\u001b[39mchromium\u001b[38;5;241m.\u001b[39mlaunch(headless\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheadless)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/playwright/async_api/_context_manager.py:46\u001b[0m, in \u001b[0;36mPlaywrightContextManager.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     45\u001b[0m     playwright_future\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m---> 46\u001b[0m playwright \u001b[38;5;241m=\u001b[39m AsyncPlaywright(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(done))\u001b[38;5;241m.\u001b[39mresult())\n\u001b[1;32m     47\u001b[0m playwright\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mException\u001b[0m: Connection closed while reading from the driver"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "#import nest_asyncio\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.chromium import AsyncChromiumLoader\n",
    "\n",
    "def scrape_with_playwright(urls, schema):\n",
    "    loader = AsyncChromiumLoader(urls)\n",
    "    #nest_asyncio.apply()\n",
    "    docs = loader.load()\n",
    "    bs_transformer = BeautifulSoupTransformer()\n",
    "    docs_transformed = bs_transformer.transform_documents(\n",
    "        docs, tags_to_extract=[\"span\"]\n",
    "    )\n",
    "    print(\"Extracting content with LLM\")\n",
    "\n",
    "    # Grab the first 1000 tokens of the site\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=1000, chunk_overlap=0\n",
    "    )\n",
    "    splits = splitter.split_documents(docs_transformed)\n",
    "\n",
    "    # Process the first split\n",
    "    extracted_content = extract(schema=schema, content=splits[0].page_content)\n",
    "    pprint.pprint(extracted_content)\n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "urls = [\"https://www.wsj.com\"]\n",
    "extracted_content = scrape_with_playwright(urls, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a8cef",
   "metadata": {
    "id": "b08a8cef"
   },
   "source": [
    "We can compare the headlines scraped to the page:\n",
    "\n",
    "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/wsj_page.png?raw=1)\n",
    "\n",
    "Looking at the [LangSmith trace](https://smith.langchain.com/public/c3070198-5b13-419b-87bf-3821cdf34fa6/r), we can see what is going on under the hood:\n",
    "\n",
    "* It's following what is explained in the [extraction](docs/use_cases/extraction).\n",
    "* We call the `information_extraction` function on the input text.\n",
    "* It will attempt to populate the provided schema from the url content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a6f11e",
   "metadata": {
    "id": "a5a6f11e"
   },
   "source": [
    "## Research automation\n",
    "\n",
    "Related to scraping, we may want to answer specific questions using searched content.\n",
    "\n",
    "We can automate the process of [web research](https://blog.langchain.dev/automating-web-research/) using a retriever, such as the `WebResearchRetriever`.\n",
    "\n",
    "![Image description](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/web_research.png?raw=1)\n",
    "\n",
    "Copy requirements [from here](https://github.com/langchain-ai/web-explorer/blob/main/requirements.txt):\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "Set `GOOGLE_CSE_ID` and `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "414f0d41",
   "metadata": {
    "id": "414f0d41"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.web_research import WebResearchRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d1ce098",
   "metadata": {
    "id": "5d1ce098"
   },
   "outputs": [],
   "source": [
    "# Vectorstore\n",
    "vectorstore = Chroma(\n",
    "    #embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# LLM\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name='us-west-2')\n",
    "model_kwargs =  { \n",
    "            \"max_tokens\": 2048,\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_k\": 40,\n",
    "            \"top_p\": 0.90,\n",
    "            #\"stop_sequences\":'\\n\\n\\Human',\n",
    "        }\n",
    "    \n",
    "llm = BedrockChat(model_id='anthropic.claude-3-haiku-20240307-v1:0',client=bedrock_client, model_kwargs=model_kwargs)\n",
    "#llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Search\n",
    "search = GoogleSearchAPIWrapper(google_api_key=os.getenv(\"google_api_key\"), google_cse_id=os.getenv(\"google_cse_id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d808b9d",
   "metadata": {
    "id": "6d808b9d"
   },
   "source": [
    "Initialize retriever with the above tools to:\n",
    "    \n",
    "* Use an LLM to generate multiple relevant search queries (one LLM call)\n",
    "* Execute a search for each query\n",
    "* Choose the top K links per query  (multiple search calls in parallel)\n",
    "* Load the information from all chosen links (scrape pages in parallel)\n",
    "* Index those documents into a vectorstore\n",
    "* Find the most relevant documents for each original generated search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3e3a589",
   "metadata": {
    "id": "e3e3a589"
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "web_research_retriever = WebResearchRetriever.from_llm(\n",
    "    vectorstore=vectorstore, llm=llm, search=search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2235be61-528b-43c2-912b-a351203f0f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': ['1. What are the key components of LLM-powered autonomous agents?\\n', '2. How do LLM-based systems enable autonomous decision-making?\\n', '3. What are the underlying principles behind the functioning of LLM-powered autonomous agents?']}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What are the key components of LLM-powered autonomous agents?\\n', '2. How do LLM-based systems enable autonomous decision-making?\\n', '3. What are the underlying principles behind the functioning of LLM-powered autonomous agents?']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': \"Jun 23, 2023 ... ... Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components:\"}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'DME-Driver: Integrating Human Decision Logic and 3D Scene ...', 'link': 'https://arxiv.org/html/2401.03641v1', 'snippet': 'Jan 8, 2024 ... Key to these systems are sophisticated perception mechanisms and decision-making algorithms. These components allow vehicles to accurately\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Foundational must read GPT/LLM papers - Community - OpenAI ...', 'link': 'https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003', 'snippet': 'May 7, 2023 ... LLM Powered Autonomous Agents. Building agents with LLM (large language model) as its core controller is a cool concept. ... 3. Interesting paper\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:New URLs to load: []\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 178 ms, total: 1min 21s\n",
      "Wall time: 11.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here\\'s an overview of how LLM-powered autonomous agents work:\\n\\n1. Agent System Overview:\\n   - The agent uses a large language model (LLM) as the core controller or \"brain\".\\n   - The LLM is complemented by several key components:\\n     - Planning: Task decomposition, self-reflection to learn from past experiences.\\n     - Memory: Storing and retrieving relevant information using techniques like maximum inner product search (MIPS).\\n     - Tool Use: Interfacing with external tools, APIs, and environments to accomplish tasks.\\n\\n2. Component One: Planning\\n   - Task Decomposition: The agent breaks down high-level goals into smaller, actionable steps.\\n   - Self-Reflection: The agent reflects on its past actions and outcomes to learn and improve its planning abilities.\\n\\n3. Component Two: Memory\\n   - Types of Memory: The agent utilizes different types of memory, such as episodic memory (experiences), semantic memory (facts), and procedural memory (skills).\\n   - Maximum Inner Product Search (MIPS): An efficient technique for retrieving relevant information from the agent\\'s memory.\\n\\n4. Component Three: Tool Use\\n   - The agent can interface with external tools, APIs, and environments to gather information, perform actions, and accomplish tasks.\\n\\n5. Case Studies:\\n   - Scientific Discovery Agent: An agent that can autonomously conduct scientific research and make new discoveries.\\n   - Generative Agents Simulation: Agents that can generate creative content, such as stories, code, or designs.\\n   - Proof-of-Concept Examples: Demonstrations of LLM-powered agents performing various tasks, such as AutoGPT, GPT-Engineer, and BabyAGI.\\n\\n6. Challenges:\\n   - Finite context length: LLMs have a limited ability to remember and reason about long-term information.\\n   - Challenges in long-term planning and task decomposition: Effectively planning and exploring solutions over an extended period remains difficult.\\n   - Reliability of natural language interface: Potential issues with LLM outputs, such as formatting errors and unexpected behaviors.\\n\\nOverall, LLM-powered autonomous agents aim to leverage the powerful language understanding and generation capabilities of large language models to create intelligent, task-solving agents. However, there are still significant challenges to overcome in areas like long-term planning, memory, and reliable interaction with external tools and environments.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Retrival chain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"\"\"Your are a helpful assistant to provide omprehensive and truthful answers to questions, \\n\n",
    "                drawing upon all relevant information contained within the specified in {context}. \\n \n",
    "                You add value by analyzing the situation and offering insights to enrich your answer. \\n\n",
    "                Simply say I don't know if you can not find any evidence to match the question. \\n\n",
    "                \"\"\"),\n",
    "    #MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "]\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# Reranker\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor= FlashrankRerank(), base_retriever=web_research_retriever\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    #{\"context\": compression_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    #| RunnableParallel(answer=hub.pull(\"rlm/rag-prompt\") | chat |format_docs, question=itemgetter(\"question\") ) \n",
    "    RunnableParallel(context=compression_retriever | format_docs, question=RunnablePassthrough() )\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "result = rag_chain.invoke(user_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20655b74",
   "metadata": {
    "id": "20655b74",
    "outputId": "707c06ad-1678-4990-cb28-68b9cb4742e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.web_research:Generating questions for Google Search ...\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': ['1. What are the key components of LLM-powered autonomous agents?\\n', '2. How do LLM-based systems enable autonomous decision-making?\\n', '3. What are the underlying principles behind the functioning of LLM-powered autonomous agents?']}\n",
      "INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What are the key components of LLM-powered autonomous agents?\\n', '2. How do LLM-based systems enable autonomous decision-making?\\n', '3. What are the underlying principles behind the functioning of LLM-powered autonomous agents?']\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': \"Jun 23, 2023 ... ... Agent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent's brain, complemented by several key components:\"}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'DME-Driver: Integrating Human Decision Logic and 3D Scene ...', 'link': 'https://arxiv.org/html/2401.03641v1', 'snippet': 'Jan 8, 2024 ... Key to these systems are sophisticated perception mechanisms and decision-making algorithms. These components allow vehicles to accurately\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:Searching for relevant urls...\n",
      "INFO:langchain.retrievers.web_research:Search results: [{'title': 'Foundational must read GPT/LLM papers - Community - OpenAI ...', 'link': 'https://community.openai.com/t/foundational-must-read-gpt-llm-papers/197003', 'snippet': 'May 7, 2023 ... LLM Powered Autonomous Agents. Building agents with LLM (large language model) as its core controller is a cool concept. ... 3. Interesting paper\\xa0...'}]\n",
      "INFO:langchain.retrievers.web_research:New URLs to load: []\n",
      "INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 ms, sys: 4.01 ms, total: 41.3 ms\n",
      "Wall time: 6.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How do LLM Powered Autonomous Agents work?',\n",
       " 'answer': 'I do not have enough information to provide a detailed answer on how LLM-powered autonomous agents work. The provided content focuses on the general components and challenges of such systems, but does not go into the specifics of how they actually function. Without more detailed technical information, I cannot confidently describe the inner workings of these agents. I apologize that I cannot give a more complete answer to this question.\\n\\n',\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Run\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "user_input = \"How do LLM Powered Autonomous Agents work?\"\n",
    "\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm, retriever=web_research_retriever\n",
    ")\n",
    "result = qa_chain({\"question\": user_input})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a940df1",
   "metadata": {
    "id": "7a940df1"
   },
   "source": [
    "### Going deeper\n",
    "\n",
    "* Here's a [app](https://github.com/langchain-ai/web-explorer/tree/main) that wraps this retriever with a lighweight UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312c399e",
   "metadata": {
    "id": "312c399e"
   },
   "source": [
    "## Question answering over a website\n",
    "\n",
    "To answer questions over a specific website, you can use Apify's [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor, which can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs,\n",
    "and extract text content from the web pages.\n",
    "\n",
    "In the example below, we will deeply crawl the Python documentation of LangChain's Chat LLM models and answer a question over it.\n",
    "\n",
    "First, install the requirements\n",
    "`pip install apify-client langchain-openai langchain`\n",
    "\n",
    "Next, set `OPENAI_API_KEY` and `APIFY_API_TOKEN` in your environment variables.\n",
    "\n",
    "The full code follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b08da5e",
   "metadata": {
    "id": "9b08da5e",
    "outputId": "f28e3b76-d7e3-4e22-e463-def3939329b7"
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ApifyWrapper\n__root__\n  Did not find apify_api_token, please add an environment variable `APIFY_API_TOKEN` which contains it, or pass `apify_api_token` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ApifyWrapper\n\u001b[0;32m----> 5\u001b[0m apify \u001b[38;5;241m=\u001b[39m ApifyWrapper()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Call the Actor to obtain text from the crawled webpages\u001b[39;00m\n\u001b[1;32m      7\u001b[0m loader \u001b[38;5;241m=\u001b[39m apify\u001b[38;5;241m.\u001b[39mcall_actor(\n\u001b[1;32m      8\u001b[0m     actor_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapify/website-content-crawler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     run_input\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartUrls\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/docs/integrations/chat/\u001b[39m\u001b[38;5;124m\"\u001b[39m}]},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     ),\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ApifyWrapper\n__root__\n  Did not find apify_api_token, please add an environment variable `APIFY_API_TOKEN` which contains it, or pass `apify_api_token` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_community.utilities import ApifyWrapper\n",
    "\n",
    "apify = ApifyWrapper()\n",
    "# Call the Actor to obtain text from the crawled webpages\n",
    "loader = apify.call_actor(\n",
    "    actor_id=\"apify/website-content-crawler\",\n",
    "    run_input={\"startUrls\": [{\"url\": \"/docs/integrations/chat/\"}]},\n",
    "    dataset_mapping_function=lambda item: Document(\n",
    "        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create a vector store based on the crawled data\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "# Query the vector store\n",
    "query = \"Are any OpenAI chat models integrated in LangChain?\"\n",
    "result = index.query(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78d92f-bab6-4d90-bd64-37b0af255b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dui",
   "language": "python",
   "name": "dui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
