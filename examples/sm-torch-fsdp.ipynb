{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65020424",
   "metadata": {},
   "source": [
    "# Data Parallel on Amazon SageMaker Training with PyTorch Lightning and Warm Pools\n",
    "In this lab we'll write our own deep neural network using PyTorch Lightning, and train this on Amazon SageMaker using multiple GPUs. For more details [see our blog post here.](https://aws.amazon.com/blogs/machine-learning/run-pytorch-lightning-and-native-pytorch-ddp-on-amazon-sagemaker-training-featuring-amazon-search/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951bd9c",
   "metadata": {},
   "source": [
    "### Step 0. Update the SageMaker Python SDK and AWS botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b157cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade sagemaker\n",
    "%pip install boto3 --upgrade\n",
    "%pip install botocore --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67a4f2ca-8267-47ed-be54-1047827287af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are all set! Please enjoy the lab\n"
     ]
    }
   ],
   "source": [
    "from packaging import version\n",
    "import botocore\n",
    "\n",
    "your_botocore_version = botocore.__version__\n",
    "\n",
    "minimal_version = '1.27.90'\n",
    "\n",
    "if version.parse(your_botocore_version) >= version.parse(minimal_version):\n",
    "    print ('You are all set! Please enjoy the lab')\n",
    "else:\n",
    "    print ('Stop! Please install the packages in the cell above before continuing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c79da-ad43-4fbb-9223-137c46d5170e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put a string here so you know which jobs are yours, no puncutation or spaces\n",
    "your_user_string = 'emily'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c9fe74-5fb5-402f-b1e6-e66d8c9dd7a5",
   "metadata": {},
   "source": [
    "### Step 1. Upload a dataset to your S3 bucket\n",
    "The example script we're using points to the MNIST Data loader directly from the training instance, which completely bypasses S3. However, for the sake of argument, we'll show you how to load some sample data from your notebook into S3, and then from S3 onto the training instances. This is useful for larger datasets and storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fa7dfd0-ce89-4ab3-8bfe-c472d6f5a650",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.csv\n",
    "this,is,my,arbitrary,csv,file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a0c64d-5947-48cc-8c85-1ab53e60cd41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/alfred/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/alfred/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# optionally point to whichever bucket you have access to \n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8899841-6407-4165-a8c2-4bd83828accb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_train_path = 's3://{}/data/mnist/'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84a2b3a0-235c-4682-a51b-21b7a515d85c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train.csv to s3://sagemaker-us-west-2-976939723775/data/mnist/train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp train.csv {s3_train_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf242fda",
   "metadata": {},
   "source": [
    "### Step 2. Write train script and requirements into a local directory, here named `scripts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a4ecc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6645e470",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/requirements.txt\n",
    "pytorch-lightning == 1.6.3\n",
    "lightning-bolts == 0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f7dbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/mnist.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "from pytorch_lightning.plugins.environments.lightning_environment import LightningEnvironment\n",
    "from pl_bolts.datamodules.mnist_datamodule import MNISTDataModule\n",
    "\n",
    "import argparse\n",
    "\n",
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim: int = 128, learning_rate: float = 0.0001):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)\n",
    "        self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        probs = self(x)\n",
    "        acc = self.accuracy(probs, y)\n",
    "        return acc\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        acc = self.accuracy(logits, y)\n",
    "        return acc\n",
    "\n",
    "    def accuracy(self, logits, y):\n",
    "        acc = (logits.argmax(dim=-1) == y).float().mean()\n",
    "        return acc\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> None:\n",
    "\n",
    "        self.log(\"val_acc\", torch.stack(outputs).mean(), prog_bar=True)\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> None:\n",
    "        self.log(\"test_acc\", torch.stack(outputs).mean())\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    \n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--hosts\", type=list, default=os.environ[\"SM_HOSTS\"])\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--train-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=int(os.environ[\"SM_NUM_GPUS\"]))\n",
    "\n",
    "    parser.add_argument(\"--num_nodes\", type=int, default = len(os.environ[\"SM_HOSTS\"]))\n",
    "           \n",
    "    # num gpus is per node\n",
    "    world_size = int(os.environ[\"SM_NUM_GPUS\"]) * len(os.environ[\"SM_HOSTS\"])\n",
    "                 \n",
    "    parser.add_argument(\"--world-size\", type=int, default=world_size)\n",
    "    \n",
    "    parser.add_argument(\"--batch_size\", type=int, default=int(os.environ[\"SM_HP_BATCH_SIZE\"]))  \n",
    "    \n",
    "    parser.add_argument(\"--epochs\", type=int, default=int(os.environ[\"SM_HP_EPOCHS\"]))\n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    args = parse_args()\n",
    "    \n",
    "    cmd = 'ls {}'.format(args.train_dir)\n",
    "    \n",
    "    print ('Here is sample arbitrary csv train file!')\n",
    "    \n",
    "    os.system(cmd)\n",
    "    \n",
    "    dm = MNISTDataModule(batch_size=args.batch_size)\n",
    "    \n",
    "    model = LitClassifier()\n",
    "    \n",
    "    local_rank = os.environ[\"LOCAL_RANK\"]\n",
    "    torch.cuda.set_device(int(local_rank))\n",
    "    \n",
    "    num_nodes = args.num_nodes\n",
    "    num_gpus = args.num_gpus\n",
    "    \n",
    "    env = LightningEnvironment()\n",
    "    \n",
    "    env.world_size = lambda: int(os.environ.get(\"WORLD_SIZE\", 0))\n",
    "    env.global_rank = lambda: int(os.environ.get(\"RANK\", 0))\n",
    "    \n",
    "    ddp = DDPStrategy(cluster_environment=env, accelerator=\"gpu\")\n",
    "    \n",
    "    trainer = pl.Trainer(max_epochs=args.epochs, strategy=ddp, devices=num_gpus, num_nodes=num_nodes, default_root_dir = args.model_dir)\n",
    "    trainer.fit(model, datamodule=dm)\n",
    "    trainer.test(model, datamodule=dm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081b87f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3. Configure the SageMaker Training Estimator\n",
    "In this step you are using the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) as a wrapper around the core api, `create-training-job`, [as described here.](https://docs.aws.amazon.com/cli/latest/reference/sagemaker/create-training-job.html)\n",
    "\n",
    "Read more about [training on SageMaker here,](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html) with distributed training details [here.](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html)\n",
    "\n",
    "\n",
    "You can see [instance details for SageMaker here,](https://aws.amazon.com/sagemaker/pricing/) along with [instance specs from EC2 directly here.](https://aws.amazon.com/ec2/instance-types/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3d7a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/alfred/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "#role = sagemaker.get_execution_role()\n",
    "role = \"arn:aws:iam::976939723775:role/service-role/AmazonSageMaker-ExecutionRole-20210317T133000\"\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# hard code point to the DLC images\n",
    "image_uri = '763104351884.dkr.ecr.{}.amazonaws.com/pytorch-training:1.13.1-gpu-py39-cu117-ubuntu20.04-sagemaker'.format(region)\n",
    "\n",
    "hyperparameters={\n",
    "        \"fsdp\": \"'full_shard auto_wrap'\", # fully sharded data parallelism\n",
    "        #\"fsdp_transformer_layer_cls_to_wrap\": \"GPTNeoXLayer\", # transformer layer to wrap if needed\n",
    "        \"batch_size\":32, \n",
    "        \"epochs\":300,\n",
    "     }\n",
    "\n",
    "estimator = PyTorch(\n",
    "  entry_point=\"mnist.py\",\n",
    "  base_job_name=\"{}-ddp-mnist\".format(\"test1\"),\n",
    "  image_uri = image_uri,\n",
    "  role=role,\n",
    "  source_dir=\"scripts\",\n",
    "  # configures the SageMaker training resource, you can increase as you need\n",
    "  instance_count=1,\n",
    "  instance_type=\"ml.g4dn.12xlarge\",\n",
    "  py_version=\"py39\",\n",
    "  sagemaker_session=sagemaker_session,\n",
    "  distribution={\"pytorchddp\":{\"enabled\": True}},\n",
    "  debugger_hook_config=False,\n",
    "  #profiler_config=profiler_config,\n",
    "  hyperparameters=hyperparameters,\n",
    "  # enable warm pools for 20 minutes\n",
    "  keep_alive_period_in_seconds = 20 *60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50bcc311",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: test1-ddp-mnist-2024-03-13-23-42-43-548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2024-03-13 23:42:43 Starting - Starting the training job......\n",
      "2024-03-13 23:43:18 Starting - Preparing the instances for training...\n",
      "2024-03-13 23:44:05 Downloading - Downloading input data...\n",
      "2024-03-13 23:44:18 Downloading - Downloading the training image...............\n",
      "2024-03-13 23:47:09 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-03-13 23:47:55,361 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-03-13 23:47:55,417 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-13 23:47:55,428 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:47:55,430 sagemaker_pytorch_container.training INFO     Invoking SMDataParallel for native PT DDP job\u001b[0m\n",
      "\u001b[34m2024-03-13 23:47:55,430 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:47:56,584 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.9 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning==1.6.3 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pytorch-lightning==1.6.3 from https://files.pythonhosted.org/packages/bf/c4/955c35600631894e5a44d2e297367bc6d468062e5fef668c2d11fb354f53/pytorch_lightning-1.6.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.6.3-py3-none-any.whl.metadata (33 kB)\u001b[0m\n",
      "\u001b[34mCollecting lightning-bolts==0.5.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for lightning-bolts==0.5.0 from https://files.pythonhosted.org/packages/38/d0/8c7a8ea48908340a9d28019e20f70393f77cf4a56993d0af5da8e1779f35/lightning_bolts-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading lightning_bolts-0.5.0-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.8.* in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard>=2.2.0 (from pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard>=2.2.0 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics>=0.4.1 (from pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for torchmetrics>=0.4.1 from https://files.pythonhosted.org/packages/cd/23/4bb4c1b78b57682a1309974a29bfdcbfa6fcf5476e698a4f0f22affa3799/torchmetrics-1.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyDeprecate<0.4.0,>=0.3.1 (from pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pyDeprecate<0.4.0,>=0.3.1 from https://files.pythonhosted.org/packages/40/9c/173f3cf770e66f3c9592318806aebb8617ba405d6d4c09493dabea75985c/pyDeprecate-0.3.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (4.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp!=4.0.0a0,!=4.0.0a1 from https://files.pythonhosted.org/packages/bf/63/bff4168e4be6da032225fe3f2492b4627bf9416a62e58b7e9dc98c6280b2/aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/50/54/827c21cbe1187f2e39c041eeeb545f3bf327b19fadb5b97b8395f3883b25/grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/42/f4/f0031854de10a0bc7821ef9fca0b92ca0d7aa6fbfbf504c5473ba825e49c/Markdown-3.5.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (2.3.6)\u001b[0m\n",
      "\u001b[34mCollecting lightning-utilities>=0.8.0 (from torchmetrics>=0.4.1->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for lightning-utilities>=0.8.0 from https://files.pythonhosted.org/packages/7d/84/fce34a549e2f795b3a0427e7dd40719dd4f00036e50ba58198a5a706eb75/lightning_utilities-0.10.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (23.1.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/70/b0/6f1ebdabfb604e39a0f84428986b89ab55f246b64cddaa495f2c953e1f6b/frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/39/a9/1f8d42c8103bcb1da6bb719f1bc018594b5acc8eae56b3fec4720ebee225/multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/69/ea/d7e961ea9b1b818a43b155ee512117be6ab9ab67c1e94967b2e64126e8e4/yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (6.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.6.3->-r requirements.txt (line 1)) (3.16.2)\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-1.6.3-py3-none-any.whl (584 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 584.0/584.0 kB 21.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lightning_bolts-0.5.0-py3-none-any.whl (316 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.8/316.8 kB 63.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 99.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.4/840.4 kB 98.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 38.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 112.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 122.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.5.2-py3-none-any.whl (103 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.9/103.9 kB 32.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 110.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 55.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (123 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.8/123.8 kB 30.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.3/304.3 kB 65.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDEPRECATION: pytorch-lightning 1.6.3 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboard-data-server, pyDeprecate, multidict, lightning-utilities, grpcio, frozenlist, async-timeout, absl-py, yarl, torchmetrics, markdown, aiosignal, tensorboard, aiohttp, pytorch-lightning, lightning-bolts\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-2.1.0 aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 grpcio-1.62.1 lightning-bolts-0.5.0 lightning-utilities-0.10.1 markdown-3.5.2 multidict-6.0.5 pyDeprecate-0.3.2 pytorch-lightning-1.6.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2 torchmetrics-1.3.1 yarl-1.9.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.2.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,278 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,278 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,364 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,439 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,452 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,452 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,456 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,456 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,456 sagemaker-training-toolkit INFO     Host: ['algo-1']\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,457 sagemaker-training-toolkit INFO     instance type: ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,457 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1'] Hosts: ['algo-1'] process_per_hosts: 4 num_processes: 4\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,523 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:04,539 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"sagemaker_pytorch_ddp_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 32,\n",
      "        \"epochs\": 300,\n",
      "        \"fsdp\": \"'full_shard auto_wrap'\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"test1-ddp-mnist-2024-03-13-23-42-43-548\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-976939723775/test1-ddp-mnist-2024-03-13-23-42-43-548/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":32,\"epochs\":300,\"fsdp\":\"'full_shard auto_wrap'\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g4dn.12xlarge\",\"sagemaker_pytorch_ddp_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g4dn.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-976939723775/test1-ddp-mnist-2024-03-13-23-42-43-548/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g4dn.12xlarge\",\"sagemaker_pytorch_ddp_enabled\":true},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":32,\"epochs\":300,\"fsdp\":\"'full_shard auto_wrap'\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"test1-ddp-mnist-2024-03-13-23-42-43-548\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-976939723775/test1-ddp-mnist-2024-03-13-23-42-43-548/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"32\",\"--epochs\",\"300\",\"--fsdp\",\"'full_shard auto_wrap'\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=300\u001b[0m\n",
      "\u001b[34mSM_HP_FSDP='full_shard auto_wrap'\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1 -np 4 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x NCCL_PROTO=simple smddprun /opt/conda/bin/python3.9 -m mpi4py mnist.py --batch_size 32 --epochs 300 --fsdp 'full_shard auto_wrap'\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:usage: mnist.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [-h]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--train-dir TRAIN_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--num_nodes NUM_NODES]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--world-size WORLD_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--batch_size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:       [--epochs EPOCHS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:mnist.py: error: unrecognized arguments: --fsdp full_shard auto_wrap\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:usage: mnist.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [-h]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--train-dir TRAIN_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--num_nodes NUM_NODES]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--world-size WORLD_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--batch_size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:       [--epochs EPOCHS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:mnist.py: error: unrecognized arguments: --fsdp full_shard auto_wrap\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:usage: mnist.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [-h]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--train-dir TRAIN_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--num_nodes NUM_NODES]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--world-size WORLD_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--batch_size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:       [--epochs EPOCHS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:mnist.py: error: unrecognized arguments: --fsdp full_shard auto_wrap\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:usage: mnist.py\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [-h]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--hosts HOSTS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--current-host CURRENT_HOST]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--model-dir MODEL_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--train-dir TRAIN_DIR]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--num-gpus NUM_GPUS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--num_nodes NUM_NODES]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--world-size WORLD_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--batch_size BATCH_SIZE]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:       [--epochs EPOCHS]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:mnist.py: error: unrecognized arguments: --fsdp full_shard auto_wrap\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mPrimary job  terminated normally, but 1 process returned\u001b[0m\n",
      "\u001b[34ma non-zero exit code. Per user-direction, the job has been aborted.\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mmpirun.real detected that one or more processes exited with non-zero status, thus causing\u001b[0m\n",
      "\u001b[34mthe job to be terminated. The first process to do so was:\n",
      "  Process name: [[41098,1],1]\n",
      "  Exit code:    2\u001b[0m\n",
      "\u001b[34m--------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:08,598 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:08,598 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 2 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:08,599 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:08,599 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 2\u001b[0m\n",
      "\u001b[34mErrorMessage \"mpirun.real detected that one or more processes exited with non-zero status, thus causing\n",
      " the job to be terminated. The first process to do so was\n",
      " \n",
      " Process name: [[41098,1],1]\n",
      " Exit code:    2\n",
      " --------------------------------------------------------------------------\"\u001b[0m\n",
      "\u001b[34mCommand \"mpirun --host algo-1 -np 4 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x NCCL_PROTO=simple smddprun /opt/conda/bin/python3.9 -m mpi4py mnist.py --batch_size 32 --epochs 300 --fsdp 'full_shard auto_wrap'\"\u001b[0m\n",
      "\u001b[34m2024-03-13 23:48:08,599 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2024-03-13 23:48:28 Uploading - Uploading generated training model\n",
      "2024-03-13 23:48:28 Failed - Instances not retained as a result of warmpool resource limits being exceeded\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job test1-ddp-mnist-2024-03-13-23-42-43-548: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 2\nErrorMessage \"mpirun.real detected that one or more processes exited with non-zero status, thus causing\n the job to be terminated. The first process to do so was\n \n Process name: [[41098,1],1]\n Exit code:    2\n --------------------------------------------------------------------------\"\nCommand \"mpirun --host algo-1 -np 4 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x NCCL_PROTO=simple smddprun /opt/conda/bin/python3.9 -m mpi4py mnist.py --batch_size 32 --epochs 300 --fsdp 'full_shard auto_wrap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Passing True will halt your kernel, passing False will not. Both create a training job.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# here we are defining the name of the input train channel. you can use whatever name you like! up to 20 channels per job.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:s3_train_path})\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m run_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/sagemaker/estimator.py:1314\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job\u001b[38;5;241m.\u001b[39mwait(logs\u001b[38;5;241m=\u001b[39mlogs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/sagemaker/estimator.py:2597\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mlogs_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, log_type\u001b[38;5;241m=\u001b[39mlogs)\n\u001b[1;32m   2598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/sagemaker/session.py:4963\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4942\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   4943\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4944\u001b[0m \n\u001b[1;32m   4945\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4961\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4962\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4963\u001b[0m     _logs_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboto_session, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/sagemaker/session.py:6887\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6884\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 6887\u001b[0m     _check_job_status(job_name, description, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainingJobStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6888\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   6889\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/dui/lib/python3.11/site-packages/sagemaker/session.py:6940\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   6935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   6936\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6937\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6938\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6939\u001b[0m     )\n\u001b[0;32m-> 6940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6941\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   6942\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   6943\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   6944\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job test1-ddp-mnist-2024-03-13-23-42-43-548: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 2\nErrorMessage \"mpirun.real detected that one or more processes exited with non-zero status, thus causing\n the job to be terminated. The first process to do so was\n \n Process name: [[41098,1],1]\n Exit code:    2\n --------------------------------------------------------------------------\"\nCommand \"mpirun --host algo-1 -np 4 --allow-run-as-root --tag-output --oversubscribe -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -mca plm_rsh_num_concurrent 1 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x SMDATAPARALLEL_USE_SINGLENODE=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x LD_PRELOAD=/opt/conda/lib/python3.9/site-packages/gethostname.cpython-39-x86_64-linux-gnu.so -x NCCL_PROTO=simple smddprun /opt/conda/bin/python3.9 -m mpi4py mnist.py --batch_size 32 --epochs 300 --fsdp 'full_shard auto_wrap"
     ]
    }
   ],
   "source": [
    "# Passing True will halt your kernel, passing False will not. Both create a training job.\n",
    "# here we are defining the name of the input train channel. you can use whatever name you like! up to 20 channels per job.\n",
    "estimator.fit(wait=True, inputs = {'train':s3_train_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276fbfa0-d8f6-4757-9ee4-d44c8a891964",
   "metadata": {},
   "source": [
    "### Step 4. Rerun the job with a higher batch size to increase GPU Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505418e7-6078-4aa4-a288-06bb72664b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "  entry_point=\"mnist.py\",\n",
    "  base_job_name=\"{}-ddp-mnist\".format(your_user_string),\n",
    "  image_uri = image_uri,\n",
    "  role=role,\n",
    "  source_dir=\"scripts\",\n",
    "  # configures the SageMaker training resource, you can increase as you need\n",
    "  instance_count=1,\n",
    "  instance_type=\"ml.g4dn.12xlarge\",\n",
    "  py_version=\"py38\",\n",
    "  sagemaker_session=sagemaker_session,\n",
    "  distribution={\"pytorchddp\":{\"enabled\": True}},\n",
    "  debugger_hook_config=False,\n",
    "  #max_retry_attempts=5,\n",
    "  hyperparameters={\"batch_size\":320, \"epochs\":900},\n",
    "  # turn off warm pools for this instance\n",
    "  keep_alive_period_in_seconds = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8c3a0-3e07-47cc-a14c-668398ebbb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit(wait=False, inputs = {'train':s3_train_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579d948f",
   "metadata": {},
   "source": [
    "### Step 5. Optimize!\n",
    "That's the end of this lab. However, in the real world, this is just the begining. Here are a few extra things you can do to increase model accuracy (hopefully) and decrease job runtime (certainly).\n",
    "1. **Increase throughput by adding extra nodes.** Increase the number of instances in `instance_count`, and as long as you're using some for of distribution in your training script, this will automatically copy your model over all available accelerators and handle averaging the results. This is also called ***horizontal scaling.***\n",
    "2. **Increase throughput by upgrading your instances.** In addition to (or sometimes instead of) adding extra nodes, you can increase your instance to something larger. This usually means adding more accelerators, more CPU, more memory, and more bandwidth. \n",
    "3. **Increase accuracy with hyperparameter tuning**. Another critical step is picking the right hyperparameters. You can use [Syne Tune](https://github.com/awslabs/syne-tune/blob/hf_blog_post/hf_blog_post/example_syne_tune_for_hf.ipynb) for a multi-objective tuning metric as one example. Amazon SageMaker Automatic Model Tuning now provides up to three times faster hyperparameter tuning with [Hyperband](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-automatic-model-tuning-now-provides-up-to-three-times-faster-hyperparameter-tuning-with-hyperband/).  Here's another example with [SageMaker Training Compiler to tune the batch size!](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-training-compiler/huggingface/pytorch_tune_batch_size/finding_max_batch_size_for_model_training.ipynb)\n",
    "4. **Increase accuracy by adding more parameters to your model, and using a model parallel strategy.** [This is the content of the next lab!](https://github.com/aws-samples/sagemaker-distributed-training-workshop/blob/main/2_model_parallel/smp-train-gpt-simple.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae8317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "dui",
   "language": "python",
   "name": "dui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
