{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LLaMA 2 on Amazon SageMaker\n",
    "\n",
    "In this sagemaker example, we are going to learn how to fine-tune [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) using [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). [LLaMA 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) is the next version of the [LLaMA](https://arxiv.org/abs/2302.13971). Compared to the V1 model, it is trained on more data - 2T tokens and supports context length window upto 4K tokens. Learn more about LLaMa 2 in the [\"\"]() blog post.\n",
    "\n",
    "QLoRA is an efficient finetuning technique that quantizes a pretrained language model to 4 bits and attaches small “Low-Rank Adapters” which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, QLoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft). \n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune LLaMA 13B with QLoRA on Amazon SageMaker\n",
    "4. Deploy Fine-tuned LLM on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (Q)LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n",
    "- IA3: [Infused Adapter by Inhibiting and Amplifying Inner Activations](https://arxiv.org/abs/2205.05638)\n",
    "\n",
    "\n",
    "\n",
    "### Access LLaMA 2\n",
    "\n",
    "Before we can start training we have to make sure that we accepted the license of [llama 2](https://huggingface.co/meta-llama/Llama-2-70b-hf) to be able to use it. You can accept the license by clicking on the Agree and access repository button on the model page at: \n",
    "* [LLaMa 7B](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "* [LLaMa 13B](https://huggingface.co/meta-llama/Llama-2-13b-hf)\n",
    "* [LLaMa 70B](https://huggingface.co/meta-llama/Llama-2-70b-hf)\n",
    "\n",
    "## 1. Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"transformers>=4.31.0\" \"datasets[s3]>=2.13.0\" sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access any LLaMA 2 asset we need to login into our hugging face account. We can do this by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_api_key =  os.environ.get('hf_api_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/alfred/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token {hf_api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from s3fs) (3.8.4)\n",
      "Collecting aiobotocore~=2.5.0\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec==2023.6.0\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m314.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.31.18,>=1.31.17\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m304.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiobotocore~=2.5.0->s3fs) (1.15.0)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs) (1.26.16)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs) (1.0.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/alfred/.conda/envs/openai/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs) (1.16.0)\n",
      "Installing collected packages: fsspec, aioitertools, botocore, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.11.0\n",
      "    Uninstalling fsspec-2022.11.0:\n",
      "      Successfully uninstalled fsspec-2022.11.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.165\n",
      "    Uninstalling botocore-1.29.165:\n",
      "      Successfully uninstalled botocore-1.29.165\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.26.162 requires botocore<1.30.0,>=1.29.162, but you have botocore 1.31.17 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiobotocore-2.5.4 aioitertools-0.11.0 botocore-1.31.17 fsspec-2023.6.0 s3fs-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::415275363822:role/service-role/AmazonSageMaker-ExecutionRole-20220118T174575\n",
      "sagemaker bucket: sagemaker-us-east-1-415275363822\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20220118T174575')['Role']['Arn']\n",
    "except ValueError:\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and prepare the dataset\n",
    "\n",
    "we will use the [dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k) an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155), including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"instruction\": \"What is world of warcraft\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"World of warcraft is a massive online multi player role playing game. It was released in 2004 by bizarre entertainment\"\n",
    "}\n",
    "```\n",
    "\n",
    "To load the `samsum` dataset, we use the `load_dataset()` method from the 🤗 Datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 1375\n",
      "{'url': 'https://medlineplus.gov/druginfo/meds/a620010.html', 'name': 'Crizanlizumab-tmca Injection', 'notice': '', 'prescribed': 'Crizanlizumab-tmca injection is used to reduce the number of pain crises (sudden, severe pain that may last several hours to several days) in adults and children 16 years of age and older with sickle cell disease (an inherited blood disease). Crizanlizumab-tmca is in a class of medications called monoclonal antibodies. It works by blocking certain blood cells from interacting. ', 'usage': \"Crizanlizumab-tmca injection as a solution (liquid) to be injected intravenously (into a vein) by a doctor or nurse over a period of 30 minutes. It is usually given once every 2 weeks for the first two doses and then once every 4 weeks. Crizanlizumab-tmca injection can cause serious infusion reactions, which may occur within 24 hours of receiving a dose. A doctor or nurse will watch you closely while you are receiving the infusion and after the infusion to be sure you are not having a serious reaction to the medication. If you experience any of the following symptoms, tell your doctor or nurse immediately: fever, chills, nausea, vomiting, tiredness, dizziness, sweating, rash, hives, itching, wheezing, or difficulty breathing. Ask your pharmacist or doctor for a copy of the manufacturer's information for the patient. \", 'other_uses': 'This medication may be prescribed for other uses; ask your doctor or pharmacist for more information. ', 'precautions': '', 'dietary': 'Unless your doctor tells you otherwise, continue your normal diet. ', 'forgotten': 'If you miss an appointment to receive a crizanlizumab-tmca infusion, call your doctor as soon as possible. ', 'side_effects': \"Crizanlizumab-tmca injection may cause other side effects. Call your doctor if you have any unusual problems while receiving this medication. If you experience a serious side effect, you or your doctor may send a report to the Food and Drug Administration's (FDA) MedWatch Adverse Event Reporting program online ( \", 'storage_disposal': '', 'overdose': '', 'other_info': 'Keep all appointments with your doctor and the laboratory. Before having any laboratory test, tell your doctor and the laboratory personnel that you are receiving crizanlizumab-tmca. Ask your pharmacist any questions you have about crizanlizumab-tmca. It is important for you to keep a written list of all of the prescription and nonprescription (over-the-counter) medicines you are taking, as well as any products such as vitamins, minerals, or other dietary supplements. You should bring this list with you each time you visit a doctor or if you are admitted to a hospital. It is also important information to carry with you in case of emergencies. ', 'brand_names': '', 'side_effects_2': None}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# Load .jsonl file into a Dataset\n",
    "drugs_dataset = load_dataset('json', data_files='/tmp/drugs_output.jsonl', split='train')\n",
    "\n",
    "print(f\"dataset size: {len(drugs_dataset)}\")\n",
    "print(drugs_dataset[randrange(len(drugs_dataset))])\n",
    "# Dolly dataset size: 15011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instruct tune our model we need to convert our structured examples into a collection of tasks described via instructions. We define a `formatting_function` that takes a sample and returns a string with our format instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_drugs_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "What is the clinical purpose of {sample['name']}?\n",
    "### Response\n",
    "{sample['prescribed']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "What is the appropriate dosage and frequency for a patient to take {sample['name']} medication?\n",
    "### Response\n",
    "{sample['usage']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "What are the potential adverse effects or side effects of {sample['name']} medication?\n",
    "### Response\n",
    "{sample['side_effects']}\n",
    "\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input:\n",
    "Can the {sample['name']} medication be priscribed for other usages?\n",
    "### Response:\n",
    "{sample['other_uses']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "Are there notifications patients need to be aware of about the prescription drug {sample['name']}?\n",
    "### Response\n",
    "{sample['notice']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "What are the contraindications or precautions associated with {sample['name']} medication?\n",
    "### Response\n",
    "{sample['precautions']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "Should the patient avoid any specific foods, beverages, or activities while taking {sample['name']} medication?\n",
    "### Response\n",
    "{sample['dietary']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "What should the patient do if they miss a dose of {sample['name']} medication?\n",
    "### Response\n",
    "{sample['forgotten']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "How should the the {sample['name']} medication be stored to maintain its efficacy?\n",
    "### Response\n",
    "{sample['storage_disposal']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input:\n",
    "What are the signs and recommended actions to take in the event of an overdose on {sample['name']} medication?\n",
    "### Response:\n",
    "{sample['overdose']}\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "Are there any additional info regarding {sample['name']} medication?\n",
    "### Response\n",
    "{sample['other_info']}\n",
    "\n",
    "\n",
    "### Instruction\n",
    "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
    "### Input\n",
    "What alternative treatment options including over-the-counter alternatives or supplements are available to supplyment {sample['name']} medication?\n",
    "### Response\n",
    "{sample['brand_names']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Imagine you have to spend a week on a deserted island. What are 5 items you'd bring with you and why?\n",
      "\n",
      "### Answer\n",
      "If I was stranded on a deserted island for a week the first thing I'd be sure to pack is sun screen!  My biggest fear would be exposure to the elements while trying to forage for food, build a shelter and come up with a game plan to be rescued.  My 2nd item would be some form of fishing tackle, I assume the easiest thing to catch out in the middle of the ocean would be fish. I'd be sure to bring plenty of hooks, line and artificial lures in my tackle box.  The 3rd item I'd be sure to pack would be a fire-starter.  I would choose this over matches because flint/steel work while wet where matches become useless pretty quickly when wet. The 4th item I'd bring would be a guitar, there's something very relaxing and peaceful about music while you're alone and I imagine it would help pass the time.  The last item I would bring would be a picture of my family.  That would keep me going in even the darkest of times. When the fish aren't biting or the weather isn't cooperating - knowing I have a lovely family to get back home to would provide all the motivation in the world to keep fighting!\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(dolly_dataset[randrange(len(dolly_dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "What is the clinical purpose of Miconazole Vaginal?\n",
      "### Response\n",
      "Vaginal miconazole is used to treat vaginal yeast infections in adults and children 12 years of age and older. Miconazole is in a class of antifungal medications called imidazoles. It works by stopping the growth of fungi that cause infection. \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "What is the appropriate dosage and frequency for a patient to take Miconazole Vaginal medication?\n",
      "### Response\n",
      "Vaginal miconazole comes as a cream or suppository to be inserted into the vagina. The cream may also be applied to the skin around the outside of the vagina.   The suppositories are used as a one-time dose (Monistat 1) or once a day at bedtime for 3 days in a row (Monistat 3). The vaginal cream is used once a day at bedtime for 7 days in a row (Monistat 7). The cream is used twice a day for up to 7 days, on the skin around the outside of the vagina.Follow the directions on the package or as directed by your doctor carefully, and ask your doctor or pharmacist to explain any part you do not understand. Use miconazole exactly as directed. Do not use more or less of it or use it more often than directed on the package or prescribed by your doctor. Vaginal miconazole is available without a prescription (over the counter). If this is the first time you have had vaginal itching and discomfort, talk to a doctor before using miconazole. If a doctor has told you before that you had a yeast infection and you have the same symptoms again, use the vaginal cream or suppositories as directed on the package. Do not have vaginal intercourse or use other vaginal products (such as tampons, douches, or spermicides) during your treatment. You should begin to feel better during the first three days of treatment with miconazole. If your symptoms do not improve or get worse, call your doctor. To apply the external miconazole cream, use your finger to apply a small amount of cream to the affected area of skin on the outside of the vagina. The dose should be applied when you lie down to go to bed. It works best if you do not get up after applying it except to wash your hands. You may wish to wear a sanitary napkin while using the suppositories or vaginal cream to protect your clothing against stains. Continue using miconazole vaginal cream or suppositories even if you get your period during treatment. \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "What are the potential adverse effects or side effects of Miconazole Vaginal medication?\n",
      "### Response\n",
      "If you experience a serious side effect, you or your doctor may send a report to the Food and Drug Administration's (FDA) MedWatch Adverse Event Reporting program online ( \n",
      "\n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input:\n",
      "Can the Miconazole Vaginal medication be priscribed for other usages?\n",
      "### Response:\n",
      "This medication may be prescribed for other uses; ask your doctor or pharmacist for more information. \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "Are there notifications patients need to be aware of about the prescription drug Miconazole Vaginal?\n",
      "### Response\n",
      "\n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "What are the contraindications or precautions associated with Miconazole Vaginal medication?\n",
      "### Response\n",
      "\n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "Should the patient avoid any specific foods, beverages, or activities while taking Miconazole Vaginal medication?\n",
      "### Response\n",
      "Unless your doctor tells you otherwise, continue your normal diet. \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "What should the patient do if they miss a dose of Miconazole Vaginal medication?\n",
      "### Response\n",
      "Use the missed dose as soon as you remember it. However, if it is almost time for the next dose, skip the missed dose and continue your regular dosing schedule. Do not use a double dose to make up for a missed one. \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "How should the the Miconazole Vaginal medication be stored to maintain its efficacy?\n",
      "### Response\n",
      "Keep this medication in the container it came in, tightly closed, and out of reach of children. Store it at room temperature and away from excess heat and moisture (not in the bathroom). It is important to keep all medication out of sight and reach of children as many containers (such as weekly pill minders and those for eye drops, creams, patches, and inhalers) are not child-resistant and young children can open them easily. To protect young children from poisoning, always lock safety caps and immediately place the medication in a safe location – one that is up and away and out of their sight and reach.  Unneeded medications should be disposed of in special ways to ensure that pets, children, and other people cannot consume them.  However, you should not flush this medication down the toilet. Instead, the best way to dispose of your medication is through a medicine take-back program. Talk to your pharmacist or contact your local garbage/recycling department to learn about take-back programs in your community.  See the FDA's Safe Disposal of Medicines website ( \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input:\n",
      "What are the signs and recommended actions to take in the event of an overdose on Miconazole Vaginal medication?\n",
      "### Response:\n",
      "If someone swallows miconazole vaginal, call your local poison control center at 1-800-222-1222. If the victim has collapsed or is not breathing, call local emergency services at 911. \n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "Are there any additional info regarding Miconazole Vaginal medication?\n",
      "### Response\n",
      "Keep all appointments with your doctor. Ask your pharmacist any questions you have about miconazole. If you still have symptoms of infection 7 days after starting treatment with miconazole, call your doctor. It is important for you to keep a written list of all of the prescription and nonprescription (over-the-counter) medicines you are taking, as well as any products such as vitamins, minerals, or other dietary supplements. You should bring this list with you each time you visit a doctor or if you are admitted to a hospital. It is also important information to carry with you in case of emergencies. \n",
      "\n",
      "\n",
      "### Instruction\n",
      "Formulate an instruction to address inquiries related to prescription medication, tailored for doctors, pharmacists, or patients.\n",
      "### Input\n",
      "What alternative treatment options including over-the-counter alternatives or supplements are available to supplyment Miconazole Vaginal medication?\n",
      "### Response\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(format_drugs_instruction(drugs_dataset[randrange(len(drugs_dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, to formatting our samples we also want to pack multiple samples to one sequence to have a more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfred/.conda/envs/openai/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some helper functions to pack our samples into sequences of a given length and then tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Where are the oldest oak trees in Central America?\n",
      "\n",
      "### Context\n",
      "The Cerro Arenal Natural Reserve was declared a protected area on November 4, 1991, it compromises one of 78 protected areas of Nicaragua and is managed by Ministry of the Environment and Natural Resources (MARENA).\n",
      "\n",
      "The highest point in the reserve is located inside a tropical cloud forest 1,570 meters above sea level, rain occurs 8 months annually, between May and December. Cerro Arenal Natural Reserve is one of the smallest protected areas of Nicaragua and is abundant with flora and fauna.  Many animals are present throughout the reserve, animals such as monkeys, and armadillos. The reserve also contains the oldest oak trees in Central America.\n",
      "\n",
      "### Answer\n",
      "The oldest oak trees reside in the Cerro Arenal Natural Reserve, which was officially declared a protected area on November 4, 1991.</s>\n",
      "Total number of samples: 1581\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample fir Dolly only\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(lm_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the new [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec678a45aed4c3c836834b276c51532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "38165140"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_dataset.to_json('dolly_train_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown component: user_agent_creator",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/botocore/session.py:1114\u001b[0m, in \u001b[0;36mget_component\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_available_regions\u001b[39m(\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m, service_name, partition_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maws\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_non_regional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m ):\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lists the region and endpoint names of a particular partition.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m    :type service_name: string\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m    :param service_name: Name of a service to list endpoint for (e.g., s3).\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;124;03m        This parameter accepts a service name (e.g., \"elb\") or endpoint\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;124;03m        prefix (e.g., \"elasticloadbalancing\").\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m    :type partition_name: string\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m    :param partition_name: Name of the partition to limit endpoints to.\u001b[39;00m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03m        (e.g., aws for the public AWS endpoints, aws-cn for AWS China\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03m        endpoints, aws-us-gov for AWS GovCloud (US) Endpoints, etc.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m    :type allow_non_regional: bool\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;124;03m    :param allow_non_regional: Set to True to include endpoints that are\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m         not regional endpoints (e.g., s3-external-1,\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m         fips-us-gov-west-1, etc).\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m    :return: Returns a list of endpoint names (e.g., [\"us-east-1\"]).\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     resolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_internal_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendpoint_resolver\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'user_agent_creator'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# save train_dataset to s3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msess\u001b[38;5;241m.\u001b[39mdefault_bucket()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/processed/dolly/train\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlm_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_input_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muploaded data to:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining dataset to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_input_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/datasets/arrow_dataset.py:1472\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[0;34m(self, dataset_path, fs, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\n\u001b[1;32m   1469\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to overwrite \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(dataset_path)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but a dataset can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt overwrite itself.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1470\u001b[0m         )\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1472\u001b[0m     \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;66;03m# Get json serializable state\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1476\u001b[0m     key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[key]\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     ]\n\u001b[1;32m   1484\u001b[0m }\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/fsspec/asyn.py:113\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msync_wrapper\u001b[39m(func, obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given a function, make so can be called in async or blocking contexts\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    Leave obj=None if defining within a class. Pass the instance if attaching\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    as an attribute of the instance.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/fsspec/asyn.py:98\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         timeout \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError\n\u001b[1;32m    101\u001b[0m return_result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/fsspec/asyn.py:53\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     51\u001b[0m iothread[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loop[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m _lock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/s3fs/core.py:892\u001b[0m, in \u001b[0;36mS3FileSystem._makedirs\u001b[0;34m(self, path, exist_ok)\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_makedirs\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 892\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mkdir(path, create_parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m exist_ok:\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/s3fs/core.py:877\u001b[0m, in \u001b[0;36mS3FileSystem._mkdir\u001b[0;34m(self, path, acl, create_parents, **kwargs)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m region_name:\n\u001b[1;32m    874\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreateBucketConfiguration\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocationConstraint\u001b[39m\u001b[38;5;124m\"\u001b[39m: region_name\n\u001b[1;32m    876\u001b[0m     }\n\u001b[0;32m--> 877\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_s3(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalidate_cache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvalidate_cache(bucket)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/s3fs/core.py:341\u001b[0m, in \u001b[0;36mS3FileSystem._call_s3\u001b[0;34m(self, method, *akwarglist, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_s3\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, \u001b[38;5;241m*\u001b[39makwarglist, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_session()\n\u001b[1;32m    342\u001b[0m     s3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_s3(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBucket\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    343\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(s3, method)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/s3fs/core.py:527\u001b[0m, in \u001b[0;36mS3FileSystem.set_session\u001b[0;34m(self, refresh, kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m     s3creator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mcreate_client(\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mconf, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_kwargs\n\u001b[1;32m    526\u001b[0m     )\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m s3creator\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_s3creator \u001b[38;5;241m=\u001b[39m s3creator\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# the following actually closes the aiohttp connection; use of privates\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# might break in the future, would cause exception at gc time\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/aiobotocore/session.py:27\u001b[0m, in \u001b[0;36mClientCreatorContext.__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AioBaseClient:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__aenter__\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/aiobotocore/session.py:178\u001b[0m, in \u001b[0;36mAioSession._create_client\u001b[0;34m(self, service_name, region_name, api_version, use_ssl, verify, endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, config)\u001b[0m\n\u001b[1;32m    176\u001b[0m exceptions_factory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_internal_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexceptions_factory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    177\u001b[0m config_store \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig_store\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 178\u001b[0m user_agent_creator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_component\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_agent_creator\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Session configuration values for the user agent string are applied\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# just before each client creation because they may have been modified\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# at any time between session creation and client creation.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m user_agent_creator\u001b[38;5;241m.\u001b[39mset_session_config(\n\u001b[1;32m    183\u001b[0m     session_user_agent_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent_name,\n\u001b[1;32m    184\u001b[0m     session_user_agent_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent_version,\n\u001b[1;32m    185\u001b[0m     session_user_agent_extra\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent_extra,\n\u001b[1;32m    186\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/botocore/session.py:796\u001b[0m, in \u001b[0;36mget_component\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39memit(event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit_first_non_none_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    797\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39memit(event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m first_non_none_response(responses)\n",
      "File \u001b[0;32m~/.conda/envs/openai/lib/python3.10/site-packages/botocore/session.py:1116\u001b[0m, in \u001b[0;36mget_component\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Lists the region and endpoint names of a particular partition.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m:type service_name: string\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m:return: Returns a list of endpoint names (e.g., [\"us-east-1\"]).\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m resolver \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_internal_component(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendpoint_resolver\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1116\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     service_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_service_data(service_name)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown component: user_agent_creator"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/processed/drugs/train'\n",
    "lm_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-Tune LLaMA 7B with QLoRA on Amazon SageMaker\n",
    "\n",
    "We are going to use the recently introduced method in the paper \"[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2106.09685)\" by Tim Dettmers et al. QLoRA is a new technique to reduce the memory footprint of large language models during finetuning, without sacrificing performance. The TL;DR; of how QLoRA works is: \n",
    "\n",
    "* Quantize the pretrained model to 4 bits and freezing it.\n",
    "* Attach small, trainable adapter layers. (LoRA)\n",
    "* Finetune only the adapter layers, while using the frozen quantized model for context.\n",
    "\n",
    "We prepared a [run_clm.py](./scripts/run_clm.py), which implements QLora using PEFT to train our model. The script also merges the LoRA weights into the model weights after training. That way you can use the model as a normal model without any additional code. The model will be temporally offloaded to disk, if it is too large to fit into memory.\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. \n",
    "SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "### Harwarde requirements\n",
    "\n",
    "We also ran several experiments to determine, which instance type can be used for the different model sizes. The following table shows the results of our experiments. The table shows the instance type, model size, context length, and max batch size. \n",
    "\n",
    "| Model        | Instance Type     | Max Batch Size | Context Length |\n",
    "|--------------|-------------------|----------------|----------------|\n",
    "| [LLama 7B]() | `(ml.)g5.4xlarge` | `3`            | `2048`         |\n",
    "| [LLama 13B]() | `(ml.)g5.4xlarge` | `2`            | `2048`         |\n",
    "| [LLama 70B]() | `(ml.)p4d.24xlarge` | `1++` (need to test more configs)            | `2048`         |\n",
    "\n",
    "\n",
    "> You can also use `g5.2xlarge` instead of the `g5.4xlarge` instance type, but then it is not possible to use `merge_weights` parameter, since to merge the LoRA weights into the model weights, the model needs to fit into memory. But you could save the adapter weights and merge them using [merge_adapter_weights.py](./scripts/merge_adapter_weights.py) after training.\n",
    "\n",
    "_Note: We plan to extend this list in the future. feel free to contribute your setup!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 3,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                 # batch size for training 2 for 13B and 3 for 7b with g5.4x\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = './scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/processed/drugs/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-08-16-18-08-49-2023-08-16-18-08-55-778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-08-16 18:08:56 Starting - Starting the training job."
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example for LLaMA 13B, the SageMaker training job took `31728 seconds`, which is about `8.8 hours`. The ml.g5.4xlarge instance we used costs `$2.03 per hour` for on-demand usage. As a result, the total cost for training our fine-tuned LLaMa 2 model was only ~`$18`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps \n",
    "\n",
    "You can deploy your fine-tuned LLaMA model to a SageMaker endpoint and use it for inference. Check out the [Deploy Falcon 7B & 40B on Amazon SageMaker](https://www.philschmid.de/sagemaker-falcon-llm) and [Securely deploy LLMs inside VPCs with Hugging Face and Amazon SageMaker](https://www.philschmid.de/sagemaker-llm-vpc) for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
