{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe7e91e-084c-48c0-8dd9-748f84f3259a",
   "metadata": {},
   "source": [
    "# Local deployment of LLama v2 from HF\n",
    "\n",
    "In this section, we’ll go through different approaches to running inference of the Llama2 models. Before using these models, make sure you have requested access to one of the models in the official Meta Llama 2 repositories.\n",
    "\n",
    "Note: Make sure to also fill the official Meta form. Users are provided access to the repository once both forms are filled after few hours.\n",
    "\n",
    "Using transformers\n",
    "With transformers release 4.31, one can already use Llama 2 and leverage all the tools within the HF ecosystem, such as:\n",
    "\n",
    "training and inference scripts and examples\n",
    "safe file format (safetensors)\n",
    "integrations with tools such as bitsandbytes (4-bit quantization) and PEFT (parameter efficient fine-tuning)\n",
    "utilities and helpers to run generation with the model\n",
    "mechanisms to export the models to deploy\n",
    "Make sure to be using the latest transformers release and be logged into your Hugging Face account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65a861d-0142-4cb1-9228-702809e13519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers --upgrade\n",
    "#!pip install tokenizers>=0.13.3  --upgrade\n",
    "#!pip install ipywidgets\n",
    "#!pip install xformers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bf6c6b-db52-4ac6-998f-b3a899f46328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/alfred/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Login to HuggingFace\n",
    "import os\n",
    "hf_api_key =  os.environ.get('hf_api_token')\n",
    "!huggingface-cli login --token {hf_api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b246d0f1-6692-482c-aa3a-f98e7d027f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "#model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "#model = 'TheBloke/Llama-2-13B-chat-GGML'\n",
    "model = \"/model/hf/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5857024-66ce-4c31-9465-a0df7f8db573",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alfred/.local/lib/python3.9/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Top Gun Maerick\" and \"Tomorrow Never Dies\". Do you have any recommendations of other movies I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=20,\n",
    "    top_p=0.65,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=1024,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b8d8f-12ca-49a3-a7eb-6d2c1861488f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dui",
   "language": "python",
   "name": "dui"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
